---
title: k8s网络篇一-Linux底层技术
date: 2024-08-21T18:35:54+08:00
lastmod: 2024-08-21T18:35:54+08:00
author: wantflying
authorlink: https://github.com/wantflying
cover: img/cat.jpg
categories:
  - k8s
tags:
  - 网络
  - Linux
draft: false
---

k8s 向我们提供各种网络插件、网络资源，例如 Flannel、Calico、Cilium、Weave 各种网络插件。在使用之前我们需要理解它们是如何工作的，这样才能在遇到问题时能有清晰的认识，能够知其所以然。本文将依次介绍 Linux 为我们提供的工具：namespace、veth pair、iptables、Linux bridge、tun/tap 设备、iptables、ipip、vxlan、macvlan、ipvlan，以及当前最新的 ebpf 技术，这个后续开单独模块介绍。

<!--more-->

# namespace

- **作用：** 为进程提供一个隔离的环境，从进程角度来看，整个服务器就它一个进程存在，完全独占系统资源，Linux 在各个内核版本升级过程中给我们提供了不同的 namespace 隔离，包括存储挂载、系统信号量、网络、主机名、进程、cgroup
- **原理：** Linux 使用 clone()函数创建 namespace，会在/proc/pid/ns 目录下生成符号链接，链接到对应文件，只要文件一直存在，namespace 就不会消失。通过 setns()方法可以将进程放入指定 namespace，unshare()可以将指定进程从 namespace 移出。例如我们如果要进入某个 namesapce 下，也是要先拿到 namespace 对应的文件描述符，然后通过 setns 添加进程，然后就可以执行自己的程序。至于这些函数底层原理是什么，感兴趣自行了解。

# veth pair

- **作用：** veth 就是一个虚拟以太网卡，pair 就是一对的意思，那么一对虚拟网卡就是说我们往其中一个 veth 发送数据，就会转发到另外一个 veth，要注意 veth pair 的作用仅仅是将数据从一个网卡发送到另外一个网卡，前面我们也提到 namesapce 网络空间隔离，那我们就可以把 veth 放在根命令空间和新的网络空间，实现有限制的网络访问。
- **案例：** 接下来我们将创建一对 veth pair，并测试他们之间的网络访问情况
  ![](https://raw.githubusercontent.com/wantflying/blog/main/static/img/vethpair-1.png)如上图所示，当前服务器只有一个默认的 ens160 网卡，我们可以通过 _ip link add vethname1 type veth peer name vethname2_，创建一对 veth pair，当前这一对 veth 都是在根 namespace 下，且网卡状态都是 down，也就是当前没啥用，只是单纯创建出来。
  ![](https://raw.githubusercontent.com/wantflying/blog/main/static/img/vethpair-2.png)
  接下来将两个 veth 启动并设置两个不同的网段，veth1 的 IP 地址是 10.0.2.100，veth0 的地址是 10.0.1.100。
  ![](https://raw.githubusercontent.com/wantflying/blog/main/static/img/vethpair-3.png)
  此时我们在根 namespace ping veth1 和 veth0 都可以 ping 通，路由表也可以看到分别创建了两条路由规则，这个时候其实跟 pair 没啥关系，单纯可以理解就是分别创建了两个不同的虚拟网卡，只有他们在不同的网络 namespace 才能感受到 pair 的意义。接着我们通过 ip netns 创建两个不同的命令空间，这个是 linux 为我们封装好的一个工具去操作网络 namespace，其他 namespace 的操作都需要编写相应的 c 代码。接着再把 veth0 放入 netns0 空间，veth1 放到 netns1 空间，此时我们再去 ping 这两个网址，已经无法 ping 通，因为他们分别处在不同的网络 namespace。此时如果进入 netns0 或者 netns1，可以看到网卡已经被重置变成 down 状态，路由表信息也是全为空的。
  ![](https://raw.githubusercontent.com/wantflying/blog/main/static/img/vethpair-4.png)
  如上图所示，我们再次把 veth0 和 veth1 启动起来，然后在 netns0 的空间下去 ping veth1，发现 ping 不通，那么问题来了不是说两个是个 pair 吗？可以互相通的？但是这是我们逻辑上的理解，那还是要强调一下 pair 的作用：**将一端网卡的数据传送到另外一端**。是不是可以看出问题所在，我们需要先将数据送到我们的网卡上，那这个原因是本地路由没有 10.0.2.0/24，所以我们的数据压根送不到我们自己的 veth0 网卡上，所以我们只要给我们的 namespace 添加一条路由即可。但是这也只是解决了报文去的问题，回的问题也需要在 netns1 上创建相应的路由。至此跨 namespace 的网络访问就可以通了，对于各自 namespace 的网络设备都是独立的，其实如果 vth0 和 veth1 如果是在同一网段，那么也就可以直接使用默认的路由规则进行访问，无须手工添加路由规则。

# Linux Bridge

- **作用：** 对于两个不同的 namespace 可以通过 veth pair 进行访问，但是当存在多个 namespace 隔离的时候，就需要一个东西来实现不同 namespace 之间进行访问，这个就是 Linux Bridge,也可以理解为她是一个二层的物理交换机，bridge 上面有不同端口，可根据 mac 地址转发到相应端口。
- **案例：** 创建一个 bridge，并绑定 veth pair，进行网络测试，总结一下就是说只要一个虚拟网卡链接上去，那它就不会再和网络协议栈通信，而是把流量交给网桥，至于网桥能不能和网络协议栈通信，那就不管了。k8s中也有应用，就是pod中放置一个veth pair中一个，另外一个veth挂载根namespace 空间的网桥上，外面的veth没有ip地址，pod内部流量通过veth pair转发到根namespace veth上，然后再转给bridge，由bridge根网络协议栈通信。至此单节点pod之间内部通信其实就可以使用这种方式完成。

```shell
#当前环境虚拟机ip是192.168.123.111，网关是192.168.123.1

## 创建一个名称为br0的网桥，并启动  也可以通过 brctl addbr br0创建
[root@node1 ~]# ip link add name br0 type bridge
[root@node1 ~]# ip link set br0 up

## 创建一个veth0和veth1的pair，启动并设置ip地址
[root@node1 ~]# ip link add veth0 type veth peer name veth1
[root@node1 ~]# ip addr add 1.2.3.101/24 dev veth0
[root@node1 ~]# ip addr add 1.2.3.102/24 dev veth1
[root@node1 ~]# ip link set veth0 up
[root@node1 ~]# ip link set veth1 up

## 将veth0绑定到br0网桥上 或者使用 brctl addif br0 veth0
[root@node1 ~]# ip link set dev veth0 master br0

## 查看bridge绑定情况
[root@node1 ~]# bridge link
7: veth0 state UP @veth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 master br0 state forwarding priority 32 cost 2

## 此时我们理解的网络拓扑图应该是这样，但是后面通过实验可以发现有些地方存在问题

网络协议栈<------> br0 <------> veth0 <------> veth1
|                                ’|’          ’|’
|---------------------------------|------------|

## 此时从veth0 ping veth1地址，会发现无法ping通，在网卡veth1抓包发现接受到报文，但是无返回
## 同样操作从veth1 ping veth0，也是同样现象
## 同时我们发现veth0的mac地址和br0 mac地址相同
##🙅‍♂️这里还是存在一些问题，可能跟我的本地环境有关系，我是用的虚拟机，也许跟内核版本有关系，反正资料说是这里会返回reply报文给veth0，然后veth0会返回给br0，但是br0并没有将数据给网络栈，因此无法获得veth1的mac地址，从而导致二层网络不通，无法不通，折腾很久，后续有机会再验证？？？？
[root@node1 ~]# ping -c 1 -I veth0 1.2.3.102
PING 1.2.3.102 (1.2.3.102) from 1.2.3.101 veth0: 56(84) bytes of data.

--- 1.2.3.102 ping statistics ---
1 packets transmitted, 0 received, 100% packet loss, time 0ms
[root@node1 ~]# tcpdump -n -i veth1
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on veth1, link-type EN10MB (Ethernet), capture size 262144 bytes
19:03:10.496023 ARP, Request who-has 1.2.3.102 tell 1.2.3.101, length 28
19:03:11.574961 ARP, Request who-has 1.2.3.102 tell 1.2.3.101, length 28
19:03:12.608699 ARP, Request who-has 1.2.3.102 tell 1.2.3.101, length 28

```
# Tun/Tap
- **为啥要这个设备？：** 先按照osi网络模型角度来思考一下，流量通过网线以二进制流形式送到我们的服务器物理网卡，然后通过我们的网络协议栈，最底层就是物理层，然后解析数据经过数据链路层转成mac头+data，然后经过网络层拆包转换成ip头+data，然后到传输层拆包变成端口+data,这个时候其实都是在内核态进行操作的，后面就到了会话层、表示层、应用层，这里就是到具体的用户态程序去处理了。到这里我们就会有一个疑问？我们能不能参与一下数据链路层、网络层的一些工作，这个时候Linux就为我们提供了Tun/Tap设备。
- **工作机制：** Tun设备是一个点对点设备，工作在三层，所以它没有mac地址，Tap设备工作在二层。Tun/Tap设备对于整个Linux内核来说都是平等的网卡，只是它们是虚拟化出来的虚拟网卡，我们的物理网卡一端连接着外部物理网络，一边连接着我们的Linux网络协议栈。而我们Tun/Tap设备一边连接网落协议栈，一边连着用户空间的字符设备，一般位置是在/dev/ney/tunX。因此用户程序可以通过操作这个字符设备来读取写入数据。可以拿vpn举个例子：互联网环境下，要远程连接到公司应用，需要安装vpn，当我们安装好之后就可以使用公司应用，我们可以分析一下公司应用流量是如何流转的，从而理解Tun/Tap设备的作用。我们假设本地IP、本地Tun设备IP、公司设备IP、公司Tun设备IP、公司服务IP。我们肯定是直接访公司服务IP，但是正常环境下肯定是无法ping通，因此当我们访问公司IP时，我们的应用通过socket和网络协议栈通信，然后把流量路由到本地Tun设备网卡，这个时候数据就会被送到对端的字符设备/dev/net/tun0，然后我们的vpn程序肯定是在监听这个设备的数据，这个时候就会收到这个数据报文data，这个时候它是一个三层的数据包，这个时候vpn设备就把它作为普通的数据包，为他添加应用层相关协议参数，然后再把数据包写入字符设备，然后送入内核空间的Tun设备，这个时候内核设备就会对它进行封包，填充公司设备ip、公司设备端口，然后路由到本地设备ip填充mac地址，然后通过互联网送到公司设备eth0网卡，然后通过内核协议栈拆包，然后可以得到我们之前生成的数据data，这个时候再把data写入公司Tun字符设备，然后交给Tun网卡，接着边根据对应的路由规则访问对应服务即可。
-  **总结：** 有了Tun/Tap虚拟网卡设备，我们可以对网络二层数据包、或者三层数据包进行二次处理，flannel中UDP模式就是使用这个机制，k8s跨节点跨Pod通信很多都是这种思想，通过在IP报文中封装IP报文,IPIP实现通信，但是这种机制也存在问题，对于大数据量数据包拆包、封包可能有性能损耗。