---
title: k8s网络武道篇一-Linux底层网络技术
date: 2024-08-21T18:35:54+08:00
lastmod: 2024-08-21T18:35:54+08:00
author: wantflying
authorlink: https://github.com/wantflying
cover: img/cat.jpg
categories:
  - k8s
tags:
  - 网络
  - Linux
draft: false
---

k8s 向我们提供各种网络插件、网络资源，例如 Flannel、Calico、Cilium、Weave 各种网络插件。在使用之前我们需要理解它们是如何工作的，这样才能在遇到问题时能有清晰的认识，能够知其所以然。本文将依次介绍 Linux 为我们提供的工具：namespace、veth pair、iptables、Linux bridge、tun/tap 设备、iptables、ipip、vxlan、macvlan、ipvlan，以及当前最新的 ebpf 技术，这个后续开单独模块介绍。如部分细节描述不准确，烦请指正🌾

<!--more-->

# namespace

- **作用：** 为进程提供一个隔离的环境，从进程角度来看，整个服务器就它一个进程存在，完全独占系统资源，Linux 在各个内核版本升级过程中给我们提供了不同的 namespace 隔离，包括存储挂载、系统信号量、网络、主机名、进程、cgroup
- **原理：** Linux 使用 clone()函数创建 namespace，会在/proc/pid/ns 目录下生成符号链接，链接到对应文件，只要文件一直存在，namespace 就不会消失。通过 setns()方法可以将进程放入指定 namespace，unshare()可以将指定进程从 namespace 移出。例如我们如果要进入某个 namesapce 下，也是要先拿到 namespace 对应的文件描述符，然后通过 setns 添加进程，然后就可以执行自己的程序。至于这些函数底层原理是什么，感兴趣自行了解。

# veth pair

- **作用：** veth 就是一个虚拟以太网卡，pair 就是一对的意思，那么一对虚拟网卡就是说我们往其中一个 veth 发送数据，就会转发到另外一个 veth，要注意 veth pair 的作用仅仅是将数据从一个网卡发送到另外一个网卡，前面我们也提到 namesapce 网络空间隔离，那我们就可以把 veth 放在根命令空间和新的网络空间，实现有限制的网络访问。
- **案例：** 接下来我们将创建一对 veth pair，并测试他们之间的网络访问情况
  ![](https://raw.githubusercontent.com/wantflying/blog/main/static/img/vethpair-1.png)如上图所示，当前服务器只有一个默认的 ens160 网卡，我们可以通过 _ip link add vethname1 type veth peer name vethname2_，创建一对 veth pair，当前这一对 veth 都是在根 namespace 下，且网卡状态都是 down，也就是当前没啥用，只是单纯创建出来。
  ![](https://raw.githubusercontent.com/wantflying/blog/main/static/img/vethpair-2.png)
  接下来将两个 veth 启动并设置两个不同的网段，veth1 的 IP 地址是 10.0.2.100，veth0 的地址是 10.0.1.100。
  ![](https://raw.githubusercontent.com/wantflying/blog/main/static/img/vethpair-3.png)
  此时我们在根 namespace ping veth1 和 veth0 都可以 ping 通，路由表也可以看到分别创建了两条路由规则，这个时候其实跟 pair 没啥关系，单纯可以理解就是分别创建了两个不同的虚拟网卡，只有他们在不同的网络 namespace 才能感受到 pair 的意义。接着我们通过 ip netns 创建两个不同的命令空间，这个是 linux 为我们封装好的一个工具去操作网络 namespace，其他 namespace 的操作都需要编写相应的 c 代码。接着再把 veth0 放入 netns0 空间，veth1 放到 netns1 空间，此时我们再去 ping 这两个网址，已经无法 ping 通，因为他们分别处在不同的网络 namespace。此时如果进入 netns0 或者 netns1，可以看到网卡已经被重置变成 down 状态，路由表信息也是全为空的。
  ![](https://raw.githubusercontent.com/wantflying/blog/main/static/img/vethpair-4.png)
  如上图所示，我们再次把 veth0 和 veth1 启动起来，然后在 netns0 的空间下去 ping veth1，发现 ping 不通，那么问题来了不是说两个是个 pair 吗？可以互相通的？但是这是我们逻辑上的理解，那还是要强调一下 pair 的作用：**将一端网卡的数据传送到另外一端**。是不是可以看出问题所在，我们需要先将数据送到我们的网卡上，那这个原因是本地路由没有 10.0.2.0/24，所以我们的数据压根送不到我们自己的 veth0 网卡上，所以我们只要给我们的 namespace 添加一条路由即可。但是这也只是解决了报文去的问题，回的问题也需要在 netns1 上创建相应的路由。至此跨 namespace 的网络访问就可以通了，对于各自 namespace 的网络设备都是独立的，其实如果 vth0 和 veth1 如果是在同一网段，那么也就可以直接使用默认的路由规则进行访问，无须手工添加路由规则。

# Linux Bridge

- **作用：** 对于两个不同的 namespace 可以通过 veth pair 进行访问，但是当存在多个 namespace 隔离的时候，就需要一个东西来实现不同 namespace 之间进行访问，这个就是 Linux Bridge,也可以理解为她是一个二层的物理交换机，bridge 上面有不同端口，可根据 mac 地址转发到相应端口。
- **案例：** 创建一个 bridge，并绑定 veth pair，进行网络测试，总结一下就是说只要一个虚拟网卡链接上去，那它就不会再和网络协议栈通信，而是把流量交给网桥，至于网桥能不能和网络协议栈通信，那就不管了。k8s 中也有应用，就是 pod 中放置一个 veth pair 中一个，另外一个 veth 挂载根 namespace 空间的网桥上，外面的 veth 没有 ip 地址，pod 内部流量通过 veth pair 转发到根 namespace veth 上，然后再转给 bridge，由 bridge 根网络协议栈通信。至此单节点 pod 之间内部通信其实就可以使用这种方式完成。

```shell
#当前环境虚拟机ip是192.168.123.111，网关是192.168.123.1

## 创建一个名称为br0的网桥，并启动  也可以通过 brctl addbr br0创建
[root@node1 ~]# ip link add name br0 type bridge
[root@node1 ~]# ip link set br0 up

## 创建一个veth0和veth1的pair，启动并设置ip地址
[root@node1 ~]# ip link add veth0 type veth peer name veth1
[root@node1 ~]# ip addr add 1.2.3.101/24 dev veth0
[root@node1 ~]# ip addr add 1.2.3.102/24 dev veth1
[root@node1 ~]# ip link set veth0 up
[root@node1 ~]# ip link set veth1 up

## 将veth0绑定到br0网桥上 或者使用 brctl addif br0 veth0
[root@node1 ~]# ip link set dev veth0 master br0

## 查看bridge绑定情况
[root@node1 ~]# bridge link
7: veth0 state UP @veth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 master br0 state forwarding priority 32 cost 2

## 此时我们理解的网络拓扑图应该是这样，但是后面通过实验可以发现有些地方存在问题

网络协议栈<------> br0 <------> veth0 <------> veth1
|                                ’|’          ’|’
|---------------------------------|------------|

## 此时从veth0 ping veth1地址，会发现无法ping通，在网卡veth1抓包发现接受到报文，但是无返回
## 同样操作从veth1 ping veth0，也是同样现象
## 同时我们发现veth0的mac地址和br0 mac地址相同
##🙅‍♂️这里还是存在一些问题，可能跟我的本地环境有关系，我是用的虚拟机，也许跟内核版本有关系，反正资料说是这里会返回reply报文给veth0，然后veth0会返回给br0，但是br0并没有将数据给网络栈，因此无法获得veth1的mac地址，从而导致二层网络不通，无法不通，折腾很久，后续有机会再验证？？？？
[root@node1 ~]# ping -c 1 -I veth0 1.2.3.102
PING 1.2.3.102 (1.2.3.102) from 1.2.3.101 veth0: 56(84) bytes of data.

--- 1.2.3.102 ping statistics ---
1 packets transmitted, 0 received, 100% packet loss, time 0ms
[root@node1 ~]# tcpdump -n -i veth1
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on veth1, link-type EN10MB (Ethernet), capture size 262144 bytes
19:03:10.496023 ARP, Request who-has 1.2.3.102 tell 1.2.3.101, length 28
19:03:11.574961 ARP, Request who-has 1.2.3.102 tell 1.2.3.101, length 28
19:03:12.608699 ARP, Request who-has 1.2.3.102 tell 1.2.3.101, length 28

```

# Tun/Tap

- **为啥要这个设备？：** 先按照 osi 网络模型角度来思考一下，流量通过网线以二进制流形式送到我们的服务器物理网卡，然后通过我们的网络协议栈，最底层就是物理层，然后解析数据经过数据链路层转成 mac 头+data，然后经过网络层拆包转换成 ip 头+data，然后到传输层拆包变成端口+data,这个时候其实都是在内核态进行操作的，后面就到了会话层、表示层、应用层，这里就是到具体的用户态程序去处理了。到这里我们就会有一个疑问？我们能不能参与一下数据链路层、网络层的一些工作，这个时候 Linux 就为我们提供了 Tun/Tap 设备。
- **工作机制：** Tun 设备是一个点对点设备，工作在三层，所以它没有 mac 地址，Tap 设备工作在二层。Tun/Tap 设备对于整个 Linux 内核来说都是平等的网卡，只是它们是虚拟化出来的虚拟网卡，我们的物理网卡一端连接着外部物理网络，一边连接着我们的 Linux 网络协议栈。而我们 Tun/Tap 设备一边连接网落协议栈，一边连着用户空间的字符设备，一般位置是在/dev/ney/tunX。因此用户程序可以通过操作这个字符设备来读取写入数据。可以拿 vpn 举个例子：互联网环境下，要远程连接到公司应用，需要安装 vpn，当我们安装好之后就可以使用公司应用，我们可以分析一下公司应用流量是如何流转的，从而理解 Tun/Tap 设备的作用。我们假设本地 IP、本地 Tun 设备 IP、公司设备 IP、公司 Tun 设备 IP、公司服务 IP。我们肯定是直接访公司服务 IP，但是正常环境下肯定是无法 ping 通，因此当我们访问公司 IP 时，我们的应用通过 socket 和网络协议栈通信，然后把流量路由到本地 Tun 设备网卡，这个时候数据就会被送到对端的字符设备/dev/net/tun0，然后我们的 vpn 程序肯定是在监听这个设备的数据，这个时候就会收到这个数据报文 data，这个时候它是一个三层的数据包，这个时候 vpn 设备就把它作为普通的数据包，为他添加应用层相关协议参数，然后再把数据包写入字符设备，然后送入内核空间的 Tun 设备，这个时候内核设备就会对它进行封包，填充公司设备 ip、公司设备端口，然后路由到本地设备 ip 填充 mac 地址，然后通过互联网送到公司设备 eth0 网卡，然后通过内核协议栈拆包，然后可以得到我们之前生成的数据 data，这个时候再把 data 写入公司 Tun 字符设备，然后交给 Tun 网卡，接着边根据对应的路由规则访问对应服务即可。
- **总结：** 有了 Tun/Tap 虚拟网卡设备，我们可以对网络二层数据包、或者三层数据包进行二次处理，flannel 中 UDP 模式就是使用这个机制，k8s 跨节点跨 Pod 通信很多都是这种思想，通过在 IP 报文中封装 IP 报文,IPIP 实现通信，但是这种机制也存在问题，对于大数据量数据包拆包、封包可能有性能损耗。

# iptables

- **作用：** iptables 底层是利用 netfilter 框架，可以通过对网络流量处理设置一些 hook 函数，比如屏蔽指定网址请求、SNAT、DNAT 转换等，k8s 中 servers 的负载均衡也可以采用这种机制来实现。

```
流量通常会经过系统chain，也可以为流量设置drop、return、jump、reject指令等
默认流量如下：
prerouting-->经过路由-->forwared--->postrouting-->离开主机
               |
               |
             input--->本地进程--->output--->postrouting ---->离开主机
主要就看需要给本地进程，如果需要给本地进程，则进入input chain、output chain
这也就所说的五链，每天链路可以挂不同的表，并不是所有链上都有这些表，可以设置相应规则：
	-managle 用于修改数据包头信息
	-nat 用于修改数据包ip地址
	-filter 同于控制链路上的数据包是否放行，可以reject、drop
	-raw iptables连接是有状态的，可以通过raw去除这种跟踪机制
	-security 用于数据包上应用SELinux
```

- **常用指令：**

```shell
#展示所有链路情况，如果需要指定链路，需要在后面加上 -t
iptables -L -n

#配置允许指定ip ssh访问,如果要阻止则直接
iptables -A INPUT -s 10.20.30.40/24 -p tcp --dport 22 -j ACCEPT

#配置端口转发
iptables -t nat -A PREROUTING -i eth0 -p tcp -dport -j REDIRECT --to-port 8080

#删除所有规则
iptables -F

#规则永久生效
iptables-save

#备份与恢复
iptables-save > iptables.bak
iptables-restore < iptables.bak
```

# IPIP

- **原理：** ipip 底层就是基于我们前面说的 Tun 点对点设备实现的，ipip 也只是隧道一种模式，还有 sit 模式：用 ipv4 报文封装 ipv6 报文，ISATAP：也是一种用于封装 ipv6 的方式，GRE:定义了一种通用的方式，用于在 IP 层封装另外一个 IP 层的协议，适用于 ipv4 和 ipv6
- **实践：** 我本地准备两台 centos7 arm64 架构虚拟机，两台机器 A、B 分别是 192.168.123.111、192.168.123.114

```shell
#两台机器分别加载ipip模块
modprobe ipip
[root@node4 ~]# lsmod |grep ipip
ipip                   16384  0
tunnel4                16384  1 ipip
ip_tunnel              36864  1 ipip

#在节点A上添加tun设备，设置设备ip以及本地路由
ip tunnel add tunnel1 mode ipip remote 192.168.123.114 local 192.168.123.111
ip addr add 1.2.3.100/24 dev tunnel1
ip link set tunnel1 up
ip route add 1.2.4.0/24 dev tunnel1

#同样在节点B上添加tun设备，设置设备ip以及本地路由
ip tunnel add tunnel1 mode ipip remote 192.168.123.111 local 192.168.123.114
ip addr add 1.2.4.100/24 dev tunnel1
ip link set tunnel1 up
ip route add 1.2.3.0/24 dev tunnel1

#然后A节点ping 1.2.4.100
[root@node1 ~]# ping 1.2.4.100
PING 1.2.4.100 (1.2.4.100) 56(84) bytes of data.
64 bytes from 1.2.4.100: icmp_seq=1 ttl=64 time=0.632 ms
64 bytes from 1.2.4.100: icmp_seq=2 ttl=64 time=0.906 ms
^C
--- 1.2.4.100 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1068ms
rtt min/avg/max/mdev = 0.632/0.769/0.906/0.137 ms

```

这个时候我们去对节点 B 主机 物理网卡 ens160 抓包，得到下面图片的结果，No.10 reply 报文有两个协议号都是 4 的的报文，最里面使我们 Tun 设备 ip 地址，最外面是我们物理网卡地址，这下子好像就能明白了！！！我们先分析一下发送的流程：Tun 设备会把我们的 IP 报文标识成数据报文交给网络协议栈，然后封装物理网卡地址，然后通过物理网络传输。接受的流程就是：物理网卡接受数据包，然后交给网络协议栈解析，然后在根据里面的数据报文路由到目标 Tun 设备。
![](https://raw.githubusercontent.com/wantflying/blog/main/static/img/ipip1.png)
所以对于 tun 设备来说，你只能看到 1.2.3.0/24、1.2.4.0/24 的报文，下图我们抓包注解 A tunnel1 网卡数据包也是如此，只能看到这个两个 ip 数据报文，看不到主机 IP 报文 1.2.3.100 和 1.2.4.100 跨主机跨网段访问通过这种 ipip 模式，而不是路由模式或者通过网关转发，其实过程大概是这样。
![](https://raw.githubusercontent.com/wantflying/blog/main/static/img/ipip2.png)
# VXLAN
- **原理：** Virtual eXtensible Lan，可扩展的虚拟局域网
